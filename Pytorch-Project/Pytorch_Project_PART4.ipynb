{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch Project PART4",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAqR7nfzhMuy"
      },
      "source": [
        "# 작물 잎 사진으로 질병 분류하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKzpyUDHKlwy",
        "outputId": "79d3a064-4b45-4ef7-a8c4-bc8803a2ac44"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSP-xOPphSrG"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "original_dataset_dir = './drive/MyDrive/dataset'\n",
        "classes_list = os.listdir(original_dataset_dir)\n",
        "\n",
        "base_dir = './splitted'\n",
        "os.mkdir(base_dir)\n",
        "    \n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'val')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "for clss in classes_list:\n",
        "    os.mkdir(os.path.join(train_dir, clss))\n",
        "    os.mkdir(os.path.join(validation_dir, clss))\n",
        "    os.mkdir(os.path.join(test_dir, clss))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbvv0PwFE1JL"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        " \n",
        "class Net(nn.Module): \n",
        "  \n",
        "    def __init__(self): \n",
        "    \n",
        "        super(Net, self).__init__() \n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1) \n",
        "        self.pool = nn.MaxPool2d(2,2)  \n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  \n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)  \n",
        "\n",
        "        self.fc1 = nn.Linear(4096, 512) \n",
        "        self.fc2 = nn.Linear(512, 33) \n",
        "    \n",
        "    def forward(self, x):  \n",
        "    \n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)  \n",
        "        x = self.pool(x) \n",
        "        x = F.dropout(x, p=0.25, training=self.training) \n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x) \n",
        "        x = self.pool(x) \n",
        "        x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "        x = self.conv3(x) \n",
        "        x = F.relu(x) \n",
        "        x = self.pool(x) \n",
        "        x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "        x = x.view(-1, 4096)  \n",
        "        x = self.fc1(x) \n",
        "        x = F.relu(x) \n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.fc2(x) \n",
        "\n",
        "        return F.log_softmax(x, dim=1)  \n",
        "\n",
        "model_base = Net().to(DEVICE)  \n",
        "optimizer = optim.Adam(model_base.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ5VWc0DGqc5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EsC-aaYhStW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f56480-ac02-407b-a4d0-0e44b69d946c"
      },
      "source": [
        "import math\n",
        "\n",
        "for clss in classes_list:\n",
        "    path = os.path.join(original_dataset_dir, clss)\n",
        "    fnames = os.listdir(path)\n",
        "\n",
        "\n",
        "    train_size = math.floor(0.2*len(fnames) * 0.6)\n",
        "    validation_size = math.floor(0.2*len(fnames) * 0.2)\n",
        "    test_size = math.floor(0.2*len(fnames) * 0.2)\n",
        "\n",
        "\n",
        "    train_fnames = fnames[:train_size]\n",
        "    print('Train size(',clss,'):', len(train_fnames))\n",
        "    for fname in train_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(train_dir, clss), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
        "    print('Validation size(',clss,'):', len(validation_fnames))\n",
        "    for fname in validation_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(validation_dir, clss), fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "    test_fnames = fnames[(train_size + validation_size):(train_size + test_size + validation_size)]\n",
        "    print('Test size(',clss,'):', len(test_fnames))\n",
        "    for fname in train_fnames:\n",
        "        src = os.path.join(path, fname)\n",
        "        dst = os.path.join(os.path.join(test_dir, clss), fname)\n",
        "        shutil.copyfile(src, dst)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size( Corn___Common_rust ): 143\n",
            "Validation size( Corn___Common_rust ): 47\n",
            "Test size( Corn___Common_rust ): 47\n",
            "Train size( Corn___Cercospora_leaf_spot Gray_leaf_spot ): 120\n",
            "Validation size( Corn___Cercospora_leaf_spot Gray_leaf_spot ): 40\n",
            "Test size( Corn___Cercospora_leaf_spot Gray_leaf_spot ): 40\n",
            "Train size( Background_without_leaves ): 137\n",
            "Validation size( Background_without_leaves ): 45\n",
            "Test size( Background_without_leaves ): 45\n",
            "Train size( Apple___Cedar_apple_rust ): 120\n",
            "Validation size( Apple___Cedar_apple_rust ): 40\n",
            "Test size( Apple___Cedar_apple_rust ): 40\n",
            "Train size( Blueberry___healthy ): 180\n",
            "Validation size( Blueberry___healthy ): 60\n",
            "Test size( Blueberry___healthy ): 60\n",
            "Train size( Cherry___healthy ): 120\n",
            "Validation size( Cherry___healthy ): 40\n",
            "Test size( Cherry___healthy ): 40\n",
            "Train size( Apple___Black_rot ): 120\n",
            "Validation size( Apple___Black_rot ): 40\n",
            "Test size( Apple___Black_rot ): 40\n",
            "Train size( Apple___Apple_scab ): 120\n",
            "Validation size( Apple___Apple_scab ): 40\n",
            "Test size( Apple___Apple_scab ): 40\n",
            "Train size( Apple___healthy ): 197\n",
            "Validation size( Apple___healthy ): 65\n",
            "Test size( Apple___healthy ): 65\n",
            "Train size( Cherry___Powdery_mildew ): 126\n",
            "Validation size( Cherry___Powdery_mildew ): 42\n",
            "Test size( Cherry___Powdery_mildew ): 42\n",
            "Train size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ): 129\n",
            "Validation size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ): 43\n",
            "Test size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ): 43\n",
            "Train size( Corn___Northern_Leaf_Blight ): 120\n",
            "Validation size( Corn___Northern_Leaf_Blight ): 40\n",
            "Test size( Corn___Northern_Leaf_Blight ): 40\n",
            "Train size( Grape___Black_rot ): 141\n",
            "Validation size( Grape___Black_rot ): 47\n",
            "Test size( Grape___Black_rot ): 47\n",
            "Train size( Grape___Esca_(Black_Measles) ): 165\n",
            "Validation size( Grape___Esca_(Black_Measles) ): 55\n",
            "Test size( Grape___Esca_(Black_Measles) ): 55\n",
            "Train size( Grape___healthy ): 120\n",
            "Validation size( Grape___healthy ): 40\n",
            "Test size( Grape___healthy ): 40\n",
            "Train size( Corn___healthy ): 139\n",
            "Validation size( Corn___healthy ): 46\n",
            "Test size( Corn___healthy ): 46\n",
            "Train size( Raspberry___healthy ): 120\n",
            "Validation size( Raspberry___healthy ): 40\n",
            "Test size( Raspberry___healthy ): 40\n",
            "Train size( Potato___Late_blight ): 120\n",
            "Validation size( Potato___Late_blight ): 40\n",
            "Test size( Potato___Late_blight ): 40\n",
            "Train size( Pepper,_bell___healthy ): 177\n",
            "Validation size( Pepper,_bell___healthy ): 59\n",
            "Test size( Pepper,_bell___healthy ): 59\n",
            "Train size( Potato___Early_blight ): 120\n",
            "Validation size( Potato___Early_blight ): 40\n",
            "Test size( Potato___Early_blight ): 40\n",
            "Train size( Peach___Bacterial_spot ): 275\n",
            "Validation size( Peach___Bacterial_spot ): 91\n",
            "Test size( Peach___Bacterial_spot ): 91\n",
            "Train size( Pepper,_bell___Bacterial_spot ): 120\n",
            "Validation size( Pepper,_bell___Bacterial_spot ): 40\n",
            "Test size( Pepper,_bell___Bacterial_spot ): 40\n",
            "Train size( Soybean___healthy ): 610\n",
            "Validation size( Soybean___healthy ): 203\n",
            "Test size( Soybean___healthy ): 203\n",
            "Train size( Orange___Haunglongbing_(Citrus_greening) ): 660\n",
            "Validation size( Orange___Haunglongbing_(Citrus_greening) ): 220\n",
            "Test size( Orange___Haunglongbing_(Citrus_greening) ): 220\n",
            "Train size( Peach___healthy ): 120\n",
            "Validation size( Peach___healthy ): 40\n",
            "Test size( Peach___healthy ): 40\n",
            "Train size( Potato___healthy ): 120\n",
            "Validation size( Potato___healthy ): 40\n",
            "Test size( Potato___healthy ): 40\n",
            "Train size( Tomato___Leaf_Mold ): 120\n",
            "Validation size( Tomato___Leaf_Mold ): 40\n",
            "Test size( Tomato___Leaf_Mold ): 40\n",
            "Train size( Strawberry___Leaf_scorch ): 133\n",
            "Validation size( Strawberry___Leaf_scorch ): 44\n",
            "Test size( Strawberry___Leaf_scorch ): 44\n",
            "Train size( Tomato___Septoria_leaf_spot ): 212\n",
            "Validation size( Tomato___Septoria_leaf_spot ): 70\n",
            "Test size( Tomato___Septoria_leaf_spot ): 70\n",
            "Train size( Tomato___Spider_mites Two-spotted_spider_mite ): 201\n",
            "Validation size( Tomato___Spider_mites Two-spotted_spider_mite ): 67\n",
            "Test size( Tomato___Spider_mites Two-spotted_spider_mite ): 67\n",
            "Train size( Squash___Powdery_mildew ): 220\n",
            "Validation size( Squash___Powdery_mildew ): 73\n",
            "Test size( Squash___Powdery_mildew ): 73\n",
            "Train size( Tomato___Early_blight ): 120\n",
            "Validation size( Tomato___Early_blight ): 40\n",
            "Test size( Tomato___Early_blight ): 40\n",
            "Train size( Tomato___Bacterial_spot ): 255\n",
            "Validation size( Tomato___Bacterial_spot ): 85\n",
            "Test size( Tomato___Bacterial_spot ): 85\n",
            "Train size( Strawberry___healthy ): 120\n",
            "Validation size( Strawberry___healthy ): 40\n",
            "Test size( Strawberry___healthy ): 40\n",
            "Train size( Tomato___healthy ): 190\n",
            "Validation size( Tomato___healthy ): 63\n",
            "Test size( Tomato___healthy ): 63\n",
            "Train size( Tomato___Late_blight ): 229\n",
            "Validation size( Tomato___Late_blight ): 76\n",
            "Test size( Tomato___Late_blight ): 76\n",
            "Train size( Tomato___Tomato_mosaic_virus ): 120\n",
            "Validation size( Tomato___Tomato_mosaic_virus ): 40\n",
            "Test size( Tomato___Tomato_mosaic_virus ): 40\n",
            "Train size( Tomato___Target_Spot ): 168\n",
            "Validation size( Tomato___Target_Spot ): 56\n",
            "Test size( Tomato___Target_Spot ): 56\n",
            "Train size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ): 642\n",
            "Validation size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ): 214\n",
            "Test size( Tomato___Tomato_Yellow_Leaf_Curl_Virus ): 214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0XSByyghSv9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e6ccda9-53c8-40a9-e223-e6af6602ae6c"
      },
      "source": [
        "import torch\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device('cuda' if USE_CUDA else 'cpu')\n",
        "print(DEVICE)\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "EPOCH = 30\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "transform_base = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()])\n",
        "\n",
        "train_dataset = ImageFolder(root='./splitted/train', transform=transform_base)\n",
        "val_dataset = ImageFolder(root='./splitted/val', transform=transform_base)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1gZ7_HYWmR1"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(4096, 512)\n",
        "        self.fc2 = nn.Linear(512, 39)  # 클래스 개수 == 39였음\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "        x = x.view(-1, 4096)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "model_base = Net().to(DEVICE)\n",
        "optimizer = optim.Adam(model_base.parameters(), lr=0.001)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaLqSZ5m1f0X"
      },
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)  # 여기서 문제\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-riE-NiaCAa"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            output = model(data)\n",
        "\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100 * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1o7K9z629ZI",
        "outputId": "fb57182e-abe8-415f-e05a-3e5aba55548a"
      },
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "def train_baseline(model, train_loader, val_loader, optimizer, num_epochs=30):\n",
        "    best_acc = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        since = time.time()\n",
        "        train(model, train_loader, optimizer)\n",
        "        train_loss, train_acc = evaluate(model, train_loader)\n",
        "        val_loss, val_acc = evaluate(model, val_loader)\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        time_elapsed = time.time() - since\n",
        "        print('--------------- epoch {} ---------------'.format(epoch))\n",
        "        print('train Loss: {:.4f}, Accuracy: {:.2f}%'.format(train_loss, train_acc))\n",
        "        print('al Loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))\n",
        "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "base = train_baseline(model_base, train_loader, val_loader, optimizer, EPOCH)\n",
        "\n",
        "torch.save(base, 'baseline.pt')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------- epoch 1 ---------------\n",
            "train Loss: 3.1786, Accuracy: 19.57%\n",
            "al Loss: 3.1815, Accuracy: 18.97%\n",
            "Completed in 1m 39s\n",
            "--------------- epoch 2 ---------------\n",
            "train Loss: 2.4526, Accuracy: 34.25%\n",
            "al Loss: 2.4722, Accuracy: 33.17%\n",
            "Completed in 1m 37s\n",
            "--------------- epoch 3 ---------------\n",
            "train Loss: 1.9233, Accuracy: 47.14%\n",
            "al Loss: 1.9731, Accuracy: 46.27%\n",
            "Completed in 1m 37s\n",
            "--------------- epoch 4 ---------------\n",
            "train Loss: 1.6037, Accuracy: 55.86%\n",
            "al Loss: 1.6905, Accuracy: 52.63%\n",
            "Completed in 1m 38s\n",
            "--------------- epoch 5 ---------------\n",
            "train Loss: 1.3034, Accuracy: 63.50%\n",
            "al Loss: 1.4196, Accuracy: 60.14%\n",
            "Completed in 1m 38s\n",
            "--------------- epoch 6 ---------------\n",
            "train Loss: 1.1400, Accuracy: 67.59%\n",
            "al Loss: 1.2859, Accuracy: 63.53%\n",
            "Completed in 1m 43s\n",
            "--------------- epoch 7 ---------------\n",
            "train Loss: 1.1462, Accuracy: 65.76%\n",
            "al Loss: 1.3315, Accuracy: 60.55%\n",
            "Completed in 1m 39s\n",
            "--------------- epoch 8 ---------------\n",
            "train Loss: 0.8794, Accuracy: 74.83%\n",
            "al Loss: 1.0716, Accuracy: 69.07%\n",
            "Completed in 1m 39s\n",
            "--------------- epoch 9 ---------------\n",
            "train Loss: 0.8891, Accuracy: 73.56%\n",
            "al Loss: 1.1000, Accuracy: 67.36%\n",
            "Completed in 1m 39s\n",
            "--------------- epoch 10 ---------------\n",
            "train Loss: 0.7462, Accuracy: 77.54%\n",
            "al Loss: 0.9523, Accuracy: 70.83%\n",
            "Completed in 1m 39s\n",
            "--------------- epoch 11 ---------------\n",
            "train Loss: 0.6799, Accuracy: 79.59%\n",
            "al Loss: 0.9090, Accuracy: 72.22%\n",
            "Completed in 1m 40s\n",
            "--------------- epoch 12 ---------------\n",
            "train Loss: 0.6182, Accuracy: 82.17%\n",
            "al Loss: 0.8582, Accuracy: 72.99%\n",
            "Completed in 1m 44s\n",
            "--------------- epoch 13 ---------------\n",
            "train Loss: 0.5730, Accuracy: 82.53%\n",
            "al Loss: 0.8169, Accuracy: 74.79%\n",
            "Completed in 1m 41s\n",
            "--------------- epoch 14 ---------------\n",
            "train Loss: 0.7603, Accuracy: 76.71%\n",
            "al Loss: 1.0422, Accuracy: 66.87%\n",
            "Completed in 1m 41s\n",
            "--------------- epoch 15 ---------------\n",
            "train Loss: 0.4257, Accuracy: 88.21%\n",
            "al Loss: 0.6967, Accuracy: 78.42%\n",
            "Completed in 1m 41s\n",
            "--------------- epoch 16 ---------------\n",
            "train Loss: 0.4753, Accuracy: 85.87%\n",
            "al Loss: 0.7657, Accuracy: 76.13%\n",
            "Completed in 1m 40s\n",
            "--------------- epoch 17 ---------------\n",
            "train Loss: 0.3720, Accuracy: 89.24%\n",
            "al Loss: 0.6520, Accuracy: 79.93%\n",
            "Completed in 1m 40s\n",
            "--------------- epoch 18 ---------------\n",
            "train Loss: 0.3433, Accuracy: 90.77%\n",
            "al Loss: 0.6267, Accuracy: 80.42%\n",
            "Completed in 1m 40s\n",
            "--------------- epoch 19 ---------------\n",
            "train Loss: 0.3061, Accuracy: 91.33%\n",
            "al Loss: 0.6205, Accuracy: 80.91%\n",
            "Completed in 1m 40s\n",
            "--------------- epoch 20 ---------------\n",
            "train Loss: 0.3304, Accuracy: 90.99%\n",
            "al Loss: 0.6510, Accuracy: 80.05%\n",
            "Completed in 1m 41s\n",
            "--------------- epoch 21 ---------------\n",
            "train Loss: 0.2706, Accuracy: 92.32%\n",
            "al Loss: 0.6193, Accuracy: 80.78%\n",
            "Completed in 1m 40s\n",
            "--------------- epoch 22 ---------------\n",
            "train Loss: 0.2782, Accuracy: 92.05%\n",
            "al Loss: 0.6256, Accuracy: 80.62%\n",
            "Completed in 1m 40s\n",
            "--------------- epoch 23 ---------------\n",
            "train Loss: 0.3134, Accuracy: 89.97%\n",
            "al Loss: 0.6808, Accuracy: 77.93%\n",
            "Completed in 1m 40s\n",
            "--------------- epoch 24 ---------------\n",
            "train Loss: 0.2051, Accuracy: 94.63%\n",
            "al Loss: 0.5384, Accuracy: 83.03%\n",
            "Completed in 1m 40s\n",
            "--------------- epoch 25 ---------------\n",
            "train Loss: 0.2255, Accuracy: 93.43%\n",
            "al Loss: 0.6078, Accuracy: 80.62%\n",
            "Completed in 1m 40s\n",
            "--------------- epoch 26 ---------------\n",
            "train Loss: 0.2016, Accuracy: 94.57%\n",
            "al Loss: 0.5692, Accuracy: 82.95%\n",
            "Completed in 1m 41s\n",
            "--------------- epoch 27 ---------------\n",
            "train Loss: 0.1871, Accuracy: 95.20%\n",
            "al Loss: 0.5582, Accuracy: 82.33%\n",
            "Completed in 1m 41s\n",
            "--------------- epoch 28 ---------------\n",
            "train Loss: 0.1638, Accuracy: 95.62%\n",
            "al Loss: 0.5508, Accuracy: 83.52%\n",
            "Completed in 1m 41s\n",
            "--------------- epoch 29 ---------------\n",
            "train Loss: 0.1454, Accuracy: 96.43%\n",
            "al Loss: 0.5356, Accuracy: 82.95%\n",
            "Completed in 1m 41s\n",
            "--------------- epoch 30 ---------------\n",
            "train Loss: 0.1858, Accuracy: 95.05%\n",
            "al Loss: 0.6187, Accuracy: 81.07%\n",
            "Completed in 1m 41s\n"
          ]
        }
      ]
    }
  ]
}